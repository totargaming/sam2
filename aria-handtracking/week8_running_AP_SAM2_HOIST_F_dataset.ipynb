{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7847
        },
        "collapsed": true,
        "id": "zxKDbPNiEmlu",
        "outputId": "a648393b-d12a-4af5-f9d7-79d15e6e121e"
      },
      "outputs": [],
      "source": [
        "# Get polygon and Visualize the mask for SAM2\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Load the file\n",
        "file_path = 'mask_data_box(1).json'  # Replace with your actual file path\n",
        "with open(file_path, 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "for frame in data:\n",
        "  # Extract polygons from the data\n",
        "  polygons = data[frame][0]['polygons']\n",
        "  # print(polygons)\n",
        "\n",
        "  # Adjust the coordinates to make the mask horizontal\n",
        "  fig, ax = plt.subplots(figsize=(8.54, 4.8))  # Adjust figure size to match the image dimensions\n",
        "\n",
        "  for polygon in polygons:\n",
        "      polygon = np.array(polygon)\n",
        "      # Swap the x and y values to make the mask horizontal\n",
        "      polygon = polygon[:, [1, 0]]\n",
        "      ax.fill(polygon[:, 0], polygon[:, 1], edgecolor='black', fill=\"Green\")  # Fill with no color and black edges\n",
        "\n",
        "  ax.set_aspect('equal', adjustable='box')\n",
        "  ax.set_xlim(0, 854)  # Set x-axis limit\n",
        "  ax.set_ylim(0, 480)  # Set y-axis limit\n",
        "  plt.title('Polygons Visualization')\n",
        "  plt.gca().invert_yaxis()  # Invert the y-axis to match the image coordinate system\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For merging \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV files\n",
        "hand_masks_df = pd.read_csv('/home/totargaming/workspace/sam2/aria-handtracking/hand_masks_bboxes_dot.csv')\n",
        "final_df = pd.read_csv('/home/totargaming/workspace/sam2/aria-handtracking/final.csv')\n",
        "\n",
        "# Rename columns for clarity\n",
        "hand_masks_df.rename(columns={'point_x': 'hand_point_x', 'point_y': 'hand_point_y'}, inplace=True)\n",
        "final_df.rename(columns={'point_x': 'obj_point_x', 'point_y': 'obj_point_y'}, inplace=True)\n",
        "\n",
        "# Calculate bottom right coordinates for hand_masks_df\n",
        "hand_masks_df['hand_box_x2'] = hand_masks_df['box_x1'] + hand_masks_df['width']\n",
        "hand_masks_df['hand_box_y2'] = hand_masks_df['box_y1'] + hand_masks_df['height']\n",
        "\n",
        "# Calculate bottom right coordinates for final_df\n",
        "final_df['obj_box_x2'] = final_df['box_x1'] + final_df['width']\n",
        "final_df['obj_box_y2'] = final_df['box_y1'] + final_df['height']\n",
        "\n",
        "# Merge the dataframes on folder_name\n",
        "merged_df = pd.merge(final_df, hand_masks_df, on='folder_name')\n",
        "\n",
        "# Calculate the combined bounding box coordinates\n",
        "merged_df['box_x1'] = merged_df[['box_x1_x', 'box_x1_y']].min(axis=1)\n",
        "merged_df['box_y1'] = merged_df[['box_y1_x', 'box_y1_y']].min(axis=1)\n",
        "merged_df['box_x2'] = merged_df[['obj_box_x2', 'hand_box_x2']].max(axis=1)\n",
        "merged_df['box_y2'] = merged_df[['obj_box_y2', 'hand_box_y2']].max(axis=1)\n",
        "\n",
        "# Select and rename the required columns\n",
        "output_df = merged_df[['folder_name', 'hand_point_x', 'hand_point_y', 'obj_point_x', 'obj_point_y', 'box_x1', 'box_y1', 'box_x2', 'box_y2', 'frame']]\n",
        "output_df.rename(columns={'box_x2': 'box_x2', 'box_y2': 'box_y2'}, inplace=True)\n",
        "\n",
        "# Save the result to a new CSV file\n",
        "output_df.to_csv('/home/totargaming/workspace/sam2/aria-handtracking/merged_output.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "p8iuhHUWGFOk",
        "outputId": "8e4d27a6-5843-402d-d265-c36982205f30"
      },
      "outputs": [],
      "source": [
        "# Get polygon and visualize a particular video for GT mask\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from pycocotools import mask as mask_utils\n",
        "\n",
        "# Load the JSON file\n",
        "json_path = \"./HOIST/valid.json\"\n",
        "with open(json_path, \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Create mappings\n",
        "video_map = {video[\"id\"]: video[\"file_names\"] for video in data[\"videos\"]}\n",
        "video_dim_map = {video[\"id\"]: (video[\"height\"], video[\"width\"]) for video in data[\"videos\"]}\n",
        "\n",
        "# Output directory\n",
        "output_folder = \"output_masks\"\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# Iterate through all videos\n",
        "for video_id, frame_files in video_map.items():\n",
        "  if video_id == 50:\n",
        "    frame_height, frame_width = video_dim_map.get(video_id, (720, 1280))  # Default dimensions if not found\n",
        "\n",
        "    # Find all annotations related to this video\n",
        "    video_annotations = [ann for ann in data[\"annotations\"] if ann[\"video_id\"] == video_id]\n",
        "\n",
        "    # Iterate through each frame\n",
        "    for frame_idx, frame_name in enumerate(frame_files):\n",
        "        # Create a blank image\n",
        "        img = np.zeros((frame_height, frame_width, 3), dtype=np.uint8)\n",
        "\n",
        "        # Apply segmentation masks for this frame\n",
        "        for annotation in video_annotations:\n",
        "            segmentation = annotation[\"segmentations\"]\n",
        "\n",
        "            if isinstance(segmentation, list) and len(segmentation) > frame_idx:\n",
        "                seg = segmentation[frame_idx]  # Get segmentation for the current frame\n",
        "                if isinstance(seg, dict) and \"counts\" in seg:\n",
        "                    mask = mask_utils.decode(mask_utils.frPyObjects(seg, frame_height, frame_width))\n",
        "                    img[mask > 0] = (0, 255, 0)  # Overlay mask in green\n",
        "\n",
        "        # Display the masked frame\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "        plt.title(f\"Video {video_id} | Frame {frame_idx}\")\n",
        "        plt.axis(\"off\")\n",
        "        plt.show()\n",
        "\n",
        "        # Save the masked frame\n",
        "        output_path = os.path.join(output_folder, frame_name.replace(\"/\", \"_\"))\n",
        "        cv2.imwrite(output_path, img)\n",
        "\n",
        "print(f\"All frames processed and saved in '{output_folder}'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KZEEgbMao8Dg",
        "outputId": "a2ad8fd7-9672-4eef-9452-1548df678cac"
      },
      "outputs": [],
      "source": [
        "# Compute AP and AP 50 (GT vs SAM2)\n",
        "import json\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from pycocotools import mask as mask_utils\n",
        "from sklearn.metrics import average_precision_score\n",
        "import os, sys\n",
        "\n",
        "# Load the JSON file for GT data\n",
        "json_path = \"valid.json\"  # Replace with your actual path\n",
        "with open(json_path, \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "def compute_ap(gt_masks, pred_masks, iou_threshold=0.5):\n",
        "    # Compute Average Precision (AP) using sklearn\n",
        "    ap = average_precision_score(gt_masks.flatten() > 0, pred_masks.flatten() > 0)\n",
        "    return ap\n",
        "\n",
        "def compute_iou(gt_mask, pred_mask):\n",
        "    intersection = np.logical_and(gt_mask, pred_mask)\n",
        "    union = np.logical_or(gt_mask, pred_mask)\n",
        "\n",
        "    # Handle division by zero if union is zero\n",
        "    if np.sum(union) == 0:\n",
        "        return 0.0  # No overlap, return IoU as 0\n",
        "\n",
        "    return np.sum(intersection) / np.sum(union)\n",
        "\n",
        "def compute_ap50(gt_masks, pred_masks, iou_threshold=0.5):\n",
        "    ground_truth = []\n",
        "    predictions = []\n",
        "\n",
        "    for gt_mask, pred_mask in zip(gt_masks, pred_masks):\n",
        "        # Compute IoU for each pair of ground truth and predicted masks\n",
        "        iou = compute_iou(gt_mask, pred_mask)\n",
        "\n",
        "        # If IoU is greater than or equal to the threshold, it's a true positive\n",
        "        if iou >= iou_threshold:\n",
        "            ground_truth.append(1)  # True positive\n",
        "        else:\n",
        "            ground_truth.append(0)  # False positive\n",
        "\n",
        "        predictions.append(iou)  # Use IoU as the predicted confidence score\n",
        "\n",
        "    # Compute Average Precision (AP) using sklearn\n",
        "    ap = average_precision_score(ground_truth, predictions)\n",
        "    return ap\n",
        "\n",
        "input= \"input_dot\"\n",
        "path = f'/content/{input}'\n",
        "dirs = os.listdir( path )\n",
        "ap_list=[]\n",
        "ap50_list=[]\n",
        "from_0_25_list=[]\n",
        "from_25_50_list=[]\n",
        "from_50_75_list=[]\n",
        "from_75_100_list=[]\n",
        "\n",
        "ap50_from_0_25_list=[]\n",
        "ap50_from_25_50_list=[]\n",
        "ap50_from_50_75_list=[]\n",
        "ap50_from_75_100_list=[]\n",
        "\n",
        "ap50_70_to_80=[]\n",
        "ap50_80_to_90=[]\n",
        "\n",
        "# Print all the files and directories\n",
        "for file in dirs:\n",
        "  file_name=file.split('.')[0]\n",
        "  print(file_name)\n",
        "  sam2_file_path = f'./{input}/{file_name}.json'  # Replace with your actual SAM2 output file path\n",
        "  if file_name!= \"\":\n",
        "    with open(sam2_file_path, 'r') as f:\n",
        "      sam2_data = json.load(f)\n",
        "\n",
        "    # Search for the video ID based on the file name\n",
        "    video_id = None\n",
        "    frame_height=None\n",
        "    frame_width=None\n",
        "    for video in data['videos']:\n",
        "        if any(file_name in fname for fname in video['file_names']):\n",
        "            video_id = video['id']\n",
        "            frame_height = video['height']\n",
        "            frame_width = video['width']\n",
        "\n",
        "            break\n",
        "\n",
        "    # print(\"running file: \", file_name)\n",
        "    # print(\"id is: \", video_id)\n",
        "    # Iterate through video 50 frames\n",
        "    video_annotations = [ann for ann in data[\"annotations\"] if ann[\"video_id\"] == video_id]\n",
        "\n",
        "    # print(\"frame_files: \", frame_files)\n",
        "    # print(\"height is: \", frame_height)\n",
        "    # print(\"width is: \", frame_width)\n",
        "\n",
        "    # Initialize lists for GT and predicted masks\n",
        "    gt_masks = []\n",
        "    pred_masks = []\n",
        "    ap=[]\n",
        "    ap50=[]\n",
        "    # Iterate through each frame\n",
        "    for frame_idx, frame_name in enumerate(frame_files):\n",
        "        # Create a blank image for GT mask\n",
        "        gt_img = np.zeros((frame_height, frame_width), dtype=np.uint8)\n",
        "\n",
        "        # Apply segmentation masks for this frame from GT data\n",
        "        for annotation in video_annotations:\n",
        "            segmentation = annotation[\"segmentations\"]\n",
        "\n",
        "            if isinstance(segmentation, list) and len(segmentation) > frame_idx:\n",
        "                seg = segmentation[frame_idx]  # Get segmentation for the current frame\n",
        "                if isinstance(seg, dict) and \"counts\" in seg:\n",
        "                    mask = mask_utils.decode(mask_utils.frPyObjects(seg, frame_height, frame_width))\n",
        "                    gt_img[mask > 0] = 1  # Set ground truth pixels to 1\n",
        "\n",
        "        # Append the GT mask to the list\n",
        "        gt_masks.append(gt_img)\n",
        "\n",
        "        # Get the corresponding polygons from SAM2 output\n",
        "        sam2_polygons = sam2_data[str(frame_idx)][0]['polygons']\n",
        "\n",
        "        # Create a blank image for predicted mask from SAM2 output\n",
        "        pred_img = np.zeros((frame_height, frame_width), dtype=np.uint8)\n",
        "\n",
        "        # Draw the polygons for SAM2 output on the pred_img\n",
        "        for polygon in sam2_polygons:\n",
        "            polygon = np.array(polygon, dtype=np.int32)  # Ensure coordinates are of integer type (CV_32S)\n",
        "            polygon = polygon[:, [1, 0]]  # Swap to (x, y) format\n",
        "            cv2.fillPoly(pred_img, [polygon], 1)  # Fill the polygon region with 1\n",
        "\n",
        "        # Append the predicted mask to the list\n",
        "        pred_masks.append(pred_img)\n",
        "\n",
        "        # # Plot GT and predicted masks side by side\n",
        "        # plt.figure(figsize=(10, 5))\n",
        "\n",
        "        # # Ground Truth mask\n",
        "        # plt.subplot(1, 2, 1)\n",
        "        # plt.imshow(gt_img, cmap='gray')\n",
        "        # plt.title(f'Ground Truth - Frame {frame_idx}')\n",
        "        # plt.axis('off')\n",
        "\n",
        "        # # SAM2 predicted mask\n",
        "        # plt.subplot(1, 2, 2)\n",
        "        # plt.imshow(pred_img, cmap='gray')\n",
        "        # plt.title(f'SAM2 Prediction - Frame {frame_idx}')\n",
        "        # plt.axis('off')\n",
        "\n",
        "        # plt.show()\n",
        "        ap.append(compute_ap(gt_img, pred_img))\n",
        "        ap50.append(compute_ap50(gt_img, pred_img))\n",
        "\n",
        "    # Compute the Average Precision (AP) for this video\n",
        "    # print(\"ap list: \", ap)\n",
        "    # print(\"ap50 list: \", ap50)\n",
        "    ap = np.mean(ap)\n",
        "    ap_list.append(ap)\n",
        "    ap50 = np.mean(ap50)\n",
        "    ap50_list.append(ap50)\n",
        "    if ap > 0 and ap < 0.25 :\n",
        "      from_0_25_list.append(file_name)\n",
        "    elif ap > 0.25 and ap < 0.5:\n",
        "      from_25_50_list.append(file_name)\n",
        "    elif ap > 0.5 and ap < 0.75:\n",
        "      from_50_75_list.append(file_name)\n",
        "    elif ap > 0.75:\n",
        "      from_75_100_list.append(file_name)\n",
        "\n",
        "    if ap50 > 0 and ap50 < 0.25 :\n",
        "      ap50_from_0_25_list.append(file_name)\n",
        "    elif ap50 > 0.25 and ap50 < 0.5:\n",
        "      ap50_from_25_50_list.append(file_name)\n",
        "    elif ap50 > 0.5 and ap50 < 0.75:\n",
        "      ap50_from_50_75_list.append(file_name)\n",
        "    elif ap50 > 0.75:\n",
        "      ap50_from_75_100_list.append(file_name)\n",
        "\n",
        "    if ap50 >= 0.70 and ap50 < 0.80:\n",
        "      ap50_70_to_80.append(file_name)\n",
        "    elif ap50 >= 0.80 and ap50 < 0.90:\n",
        "      ap50_80_to_90.append(file_name)\n",
        "    # print(f\"Average Precision for Video {video_id}: {ap}\")\n",
        "    # print(f\"Average Precision 50 for Video {video_id}: {ap50}\")\n",
        "\n",
        "plt.hist(ap_list, alpha=0.5, label='ap')\n",
        "plt.hist(ap50_list, alpha=0.5, label='ap50')\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()\n",
        "print(\"AP from 0 to 25: \", from_0_25_list)\n",
        "print(\"AP from 25 to 50: \", from_25_50_list)\n",
        "print(\"AP from 50 to 75: \", from_50_75_list)\n",
        "print(\"AP from 75 to 100: \", from_75_100_list)\n",
        "print(\"Mean AP of 40 videos: \", np.mean(ap_list))\n",
        "\n",
        "print('\\n')\n",
        "print(\"AP50 from 0 to 25: \", ap50_from_0_25_list)\n",
        "print(\"AP50 from 25 to 50: \", ap50_from_25_50_list)\n",
        "print(\"AP50 from 50 to 75: \", ap50_from_50_75_list)\n",
        "print(\"AP50 from 75 to 100: \", ap50_from_75_100_list)\n",
        "print(\"Mean AP50 of 40 videos: \", np.mean(ap50_list))\n",
        "\n",
        "print('\\n')\n",
        "print('ap50 in range 70 to 80: ', ap50_70_to_80)\n",
        "print('ap50 in range 80 to 90: ', ap50_80_to_90)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AEb5PtJrJFmK",
        "outputId": "93ef1c5e-3854-4e02-8033-f419df780f86"
      },
      "outputs": [],
      "source": [
        "# Compute AP and AP 50 (GT vs SAM2) of 1 folder\n",
        "import json\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from pycocotools import mask as mask_utils\n",
        "from sklearn.metrics import average_precision_score\n",
        "import os, sys\n",
        "\n",
        "# Load the JSON file for GT data\n",
        "json_path = \"valid.json\"  # Replace with your actual path\n",
        "with open(json_path, \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "def compute_ap(gt_masks, pred_masks, iou_threshold=0.5):\n",
        "    # Compute Average Precision (AP) using sklearn\n",
        "    ap = average_precision_score(gt_masks.flatten() > 0, pred_masks.flatten() > 0)\n",
        "    return ap\n",
        "\n",
        "def compute_iou(gt_mask, pred_mask):\n",
        "    intersection = np.logical_and(gt_mask, pred_mask)\n",
        "    union = np.logical_or(gt_mask, pred_mask)\n",
        "\n",
        "    # Handle division by zero if union is zero\n",
        "    if np.sum(union) == 0:\n",
        "        return 0.0  # No overlap, return IoU as 0\n",
        "\n",
        "    return np.sum(intersection) / np.sum(union)\n",
        "\n",
        "def compute_ap50(gt_masks, pred_masks, iou_threshold=0.5):\n",
        "    ground_truth = []\n",
        "    predictions = []\n",
        "\n",
        "    for gt_mask, pred_mask in zip(gt_masks, pred_masks):\n",
        "        # Compute IoU for each pair of ground truth and predicted masks\n",
        "        iou = compute_iou(gt_mask, pred_mask)\n",
        "\n",
        "        # If IoU is greater than or equal to the threshold, it's a true positive\n",
        "        if iou >= iou_threshold:\n",
        "            ground_truth.append(1)  # True positive\n",
        "        else:\n",
        "            ground_truth.append(0)  # False positive\n",
        "\n",
        "        predictions.append(iou)  # Use IoU as the predicted confidence score\n",
        "\n",
        "    # Compute Average Precision (AP) using sklearn\n",
        "    ap = average_precision_score(ground_truth, predictions)\n",
        "    return ap\n",
        "\n",
        "# Print all the files and directories\n",
        "\n",
        "sam2_file_path = './input_box/yBun7uGa6r8_65.json'\n",
        "if file_name!= \"\":\n",
        "  with open(sam2_file_path, 'r') as f:\n",
        "    sam2_data = json.load(f)\n",
        "\n",
        "  # Search for the video ID based on the file name\n",
        "  video_id = None\n",
        "  frame_height=None\n",
        "  frame_width=None\n",
        "  for video in data['videos']:\n",
        "      if any(file_name in fname for fname in video['file_names']):\n",
        "          video_id = video['id']\n",
        "          frame_height = video['height']\n",
        "          frame_width = video['width']\n",
        "          break\n",
        "\n",
        "  video_annotations = [ann for ann in data[\"annotations\"] if ann[\"video_id\"] == video_id]\n",
        "\n",
        "  # Initialize lists for GT and predicted masks\n",
        "  gt_masks = []\n",
        "  pred_masks = []\n",
        "  ap=[]\n",
        "  ap50=[]\n",
        "  # Iterate through each frame\n",
        "  for frame_idx, frame_name in enumerate(frame_files):\n",
        "      # Create a blank image for GT mask\n",
        "      gt_img = np.zeros((frame_height, frame_width), dtype=np.uint8)\n",
        "\n",
        "      # Apply segmentation masks for this frame from GT data\n",
        "      for annotation in video_annotations:\n",
        "          segmentation = annotation[\"segmentations\"]\n",
        "\n",
        "          if isinstance(segmentation, list) and len(segmentation) > frame_idx:\n",
        "              seg = segmentation[frame_idx]  # Get segmentation for the current frame\n",
        "              if isinstance(seg, dict) and \"counts\" in seg:\n",
        "                  mask = mask_utils.decode(mask_utils.frPyObjects(seg, frame_height, frame_width))\n",
        "                  gt_img[mask > 0] = 1  # Set ground truth pixels to 1\n",
        "\n",
        "      # Append the GT mask to the list\n",
        "      gt_masks.append(gt_img)\n",
        "\n",
        "      # Get the corresponding polygons from SAM2 output\n",
        "      sam2_polygons = sam2_data[str(frame_idx)][0]['polygons']\n",
        "\n",
        "      # Create a blank image for predicted mask from SAM2 output\n",
        "      pred_img = np.zeros((frame_height, frame_width), dtype=np.uint8)\n",
        "\n",
        "      # Draw the polygons for SAM2 output on the pred_img\n",
        "      for polygon in sam2_polygons:\n",
        "          polygon = np.array(polygon, dtype=np.int32)  # Ensure coordinates are of integer type (CV_32S)\n",
        "          polygon = polygon[:, [1, 0]]  # Swap to (x, y) format\n",
        "          cv2.fillPoly(pred_img, [polygon], 1)  # Fill the polygon region with 1\n",
        "\n",
        "      # Append the predicted mask to the list\n",
        "      pred_masks.append(pred_img)\n",
        "\n",
        "      # Plot GT and predicted masks side by side\n",
        "      plt.figure(figsize=(10, 5))\n",
        "\n",
        "      # Ground Truth mask\n",
        "      plt.subplot(1, 2, 1)\n",
        "      plt.imshow(gt_img, cmap='gray')\n",
        "      plt.title(f'Ground Truth - Frame {frame_idx}')\n",
        "      plt.axis('off')\n",
        "\n",
        "      # SAM2 predicted mask\n",
        "      plt.subplot(1, 2, 2)\n",
        "      plt.imshow(pred_img, cmap='gray')\n",
        "      plt.title(f'SAM2 Prediction - Frame {frame_idx}')\n",
        "      plt.axis('off')\n",
        "\n",
        "      # plt.show()\n",
        "      ap.append(compute_ap(gt_img, pred_img))\n",
        "      ap50.append(compute_ap50(gt_img, pred_img))\n",
        "\n",
        "  # Compute the Average Precision (AP) for this video\n",
        "  print(\"ap list: \", ap)\n",
        "  print(\"ap50 list: \", ap50)\n",
        "  ap = np.mean(ap)\n",
        "  print(\"ap is: \", ap)\n",
        "  ap50 = np.mean(ap50)\n",
        "  print(\"ap50 is: \", ap50)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8j2YzGtqvxz",
        "outputId": "6562e50f-fc41-4a77-f36f-9298ec358881"
      },
      "outputs": [],
      "source": [
        "import os, sys\n",
        "\n",
        "# Open a file\n",
        "path = \"/content/input\"\n",
        "dirs = os.listdir( path )\n",
        "\n",
        "# Print all the files and directories\n",
        "for file in dirs:\n",
        "   print(file.split('.')[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_NqGsZs8udzB",
        "outputId": "5ac5a6b5-0acc-48a2-bdc9-65510cbdb9e8"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# Path to your CSV file\n",
        "csv_file_path = 'bbox.csv'\n",
        "json_file_path = './HOIST/valid.json'\n",
        "\n",
        "# Read the CSV file into a pandas DataFrame\n",
        "df = pd.read_csv(csv_file_path)\n",
        "\n",
        "with open(json_file_path, \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Initialize lists to store the new data\n",
        "box_x1 = []\n",
        "box_y1 = []\n",
        "box_x2 = []\n",
        "box_y2 = []\n",
        "frame_numbers = []\n",
        "\n",
        "for file_name in df['folder_name']:\n",
        "    video_id = None\n",
        "    for video in data['videos']:\n",
        "        if any(file_name in fname for fname in video['file_names']):\n",
        "            video_id = video['id']\n",
        "            break\n",
        "\n",
        "    if video_id is not None:\n",
        "        for annotation in data['annotations']:\n",
        "            if annotation['video_id'] == video_id:\n",
        "                for frame_number, bbox in enumerate(annotation['bboxes']):\n",
        "                    if bbox is not None:\n",
        "                        box_x1.append(bbox[0])\n",
        "                        box_y1.append(bbox[1])\n",
        "                        box_x2.append(bbox[2])\n",
        "                        box_y2.append(bbox[3])\n",
        "                        frame_numbers.append(frame_number)\n",
        "                        break\n",
        "                else:\n",
        "                    box_x1.append(None)\n",
        "                    box_y1.append(None)\n",
        "                    box_x2.append(None)\n",
        "                    box_y2.append(None)\n",
        "                    frame_numbers.append(None)\n",
        "                break\n",
        "    else:\n",
        "        box_x1.append(None)\n",
        "        box_y1.append(None)\n",
        "        box_x2.append(None)\n",
        "        box_y2.append(None)\n",
        "        frame_numbers.append(None)\n",
        "\n",
        "# Add the new data to the DataFrame\n",
        "df['box_x1'] = box_x1\n",
        "df['box_y1'] = box_y1\n",
        "df['box_x2'] = box_x2\n",
        "df['box_y2'] = box_y2\n",
        "df['frame'] = frame_numbers\n",
        "\n",
        "# Save the updated DataFrame back to the CSV file\n",
        "df.to_csv(csv_file_path, index=False)\n",
        "\n",
        "print(\"CSV file updated successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# File paths\n",
        "polygon_centers_path = '/home/totargaming/workspace/sam2/aria-handtracking/polygon_centers.csv'\n",
        "bbox_path = '/home/totargaming/workspace/sam2/aria-handtracking/bbox.csv'\n",
        "output_path = '/home/totargaming/workspace/sam2/aria-handtracking/merged_bbox.csv'\n",
        "\n",
        "# Read the CSV files into DataFrames\n",
        "polygon_centers_df = pd.read_csv(polygon_centers_path)\n",
        "bbox_df = pd.read_csv(bbox_path)\n",
        "\n",
        "# Merge the DataFrames on the 'folder_name' column\n",
        "merged_df = pd.merge(bbox_df, polygon_centers_df, on='folder_name', how='left')\n",
        "\n",
        "# Fill in the 'point_x' and 'point_y' columns in bbox_df with the values from polygon_centers_df\n",
        "merged_df['point_x'] = merged_df['point_x_y']\n",
        "merged_df['point_y'] = merged_df['point_y_y']\n",
        "\n",
        "# Drop the redundant columns\n",
        "merged_df.drop(columns=['point_x_y', 'point_y_y'], inplace=True)\n",
        "\n",
        "# Remove duplicates based on 'folder_name' and keep the first occurrence\n",
        "merged_df = merged_df.drop_duplicates(subset='folder_name', keep='first')\n",
        "\n",
        "# Write the updated DataFrame to a new CSV file\n",
        "merged_df.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"Merged CSV saved to {output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "import pandas as pd\n",
        "import json\n",
        "import numpy as np\n",
        "from pycocotools import mask as mask_utils\n",
        "\n",
        "# Path to your CSV file\n",
        "csv_file_path = 'bbox2.csv'\n",
        "json_file_path = './HOIST/valid.json'\n",
        "\n",
        "# Read the CSV file into a pandas DataFrame\n",
        "df = pd.read_csv(csv_file_path)\n",
        "\n",
        "with open(json_file_path, \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Initialize lists to store the new data\n",
        "box_x1 = []\n",
        "box_y1 = []\n",
        "box_x2 = []\n",
        "box_y2 = []\n",
        "frame_numbers = []\n",
        "x_centers = []\n",
        "y_centers = []\n",
        "\n",
        "for file_name in df['folder_name']:\n",
        "    video_id = None\n",
        "    for video in data['videos']:\n",
        "        if any(file_name in fname for fname in video['file_names']):\n",
        "            video_id = video['id']\n",
        "            break\n",
        "\n",
        "    if video_id is not None:\n",
        "        frame_height, frame_width = video_dim_map.get(video_id, (720, 1280))  # Default dimensions\n",
        "        for annotation in data['annotations']:\n",
        "            if annotation['video_id'] == video_id:\n",
        "                for frame_number, bbox in enumerate(annotation['bboxes']):\n",
        "                    if bbox is not None:\n",
        "                        box_x1_val, box_y1_val, box_x2_val, box_y2_val = bbox\n",
        "                        box_x1.append(box_x1_val)\n",
        "                        box_y1.append(box_y1_val)\n",
        "                        box_x2.append(box_x2_val)\n",
        "                        box_y2.append(box_y2_val)\n",
        "                        frame_numbers.append(frame_number)\n",
        "\n",
        "                        # Find the corresponding segmentation for this frame\n",
        "                        segmentations = annotation['segmentations']\n",
        "                        if isinstance(segmentations, list) and len(segmentations) > frame_number:\n",
        "                            seg = segmentations[frame_number]\n",
        "                            if isinstance(seg, dict) and 'counts' in seg:\n",
        "                                mask = mask_utils.decode(mask_utils.frPyObjects(seg, frame_height, frame_width))\n",
        "\n",
        "                                # Get the coordinates of the mask pixels\n",
        "                                y_coords, x_coords = np.where(mask > 0)\n",
        "\n",
        "                                if len(x_coords) > 0 and len(y_coords) > 0:\n",
        "                                    # Compute the center as the mean of coordinates\n",
        "                                    center_x = int(np.mean(x_coords))\n",
        "                                    center_y = int(np.mean(y_coords))\n",
        "\n",
        "                                    x_centers.append(center_x)\n",
        "                                    y_centers.append(center_y)\n",
        "                                else:\n",
        "                                    x_centers.append(None)\n",
        "                                    y_centers.append(None)\n",
        "                            else:\n",
        "                                x_centers.append(None)\n",
        "                                y_centers.append(None)\n",
        "                        else:\n",
        "                            x_centers.append(None)\n",
        "                            y_centers.append(None)\n",
        "                        break\n",
        "                else:\n",
        "                    box_x1.append(None)\n",
        "                    box_y1.append(None)\n",
        "                    box_x2.append(None)\n",
        "                    box_y2.append(None)\n",
        "                    frame_numbers.append(None)\n",
        "                    x_centers.append(None)\n",
        "                    y_centers.append(None)\n",
        "                break\n",
        "    else:\n",
        "        box_x1.append(None)\n",
        "        box_y1.append(None)\n",
        "        box_x2.append(None)\n",
        "        box_y2.append(None)\n",
        "        frame_numbers.append(None)\n",
        "        x_centers.append(None)\n",
        "        y_centers.append(None)\n",
        "\n",
        "# Add the new data to the DataFrame\n",
        "df['box_x1'] = box_x1\n",
        "df['box_y1'] = box_y1\n",
        "df['width'] = box_x2\n",
        "df['height'] = box_y2\n",
        "df['frame'] = frame_numbers\n",
        "df['point_x'] = x_centers\n",
        "df['point_y'] = y_centers\n",
        "\n",
        "# Save the updated DataFrame back to the CSV file\n",
        "df.to_csv(csv_file_path, index=False)\n",
        "\n",
        "print(\"CSV file updated successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "using device: cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "frame loading (JPEG): 100%|██████████| 18/18 [00:00<00:00, 52.25it/s]\n",
            "propagate in video: 100%|██████████| 18/18 [00:00<00:00, 34.22it/s]\n",
            "frame loading (JPEG): 100%|██████████| 18/18 [00:00<00:00, 53.70it/s]\n",
            "propagate in video: 100%|██████████| 18/18 [00:00<00:00, 34.61it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 1/18\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 2/18\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 3/18\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 4/18\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 5/18\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 6/18\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 7/18\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 8/18\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 9/18\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 10/18\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 11/18\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 12/18\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 13/18\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 14/18\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 15/18\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 16/18\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 17/18\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 18/18\n",
            "Total time taken: 3.51 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "frame loading (JPEG): 100%|██████████| 18/18 [00:00<00:00, 52.18it/s]\n",
            "propagate in video: 100%|██████████| 18/18 [00:00<00:00, 34.00it/s]\n",
            "frame loading (JPEG): 100%|██████████| 18/18 [00:00<00:00, 52.15it/s]\n",
            "propagate in video: 100%|██████████| 18/18 [00:00<00:00, 34.57it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 1/18\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 2/18\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 3/18\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 4/18\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 5/18\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 6/18\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 7/18\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 8/18\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 9/18\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 10/18\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 11/18\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 12/18\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 13/18\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 14/18\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 15/18\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 16/18\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 17/18\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 18/18\n",
            "Total time taken: 3.66 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "frame loading (JPEG): 100%|██████████| 18/18 [00:00<00:00, 52.76it/s]\n",
            "propagate in video: 100%|██████████| 18/18 [00:00<00:00, 34.28it/s]\n",
            "frame loading (JPEG): 100%|██████████| 18/18 [00:00<00:00, 51.42it/s]\n",
            "propagate in video: 100%|██████████| 18/18 [00:00<00:00, 34.56it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mask shape: (1, 480, 862), mask_binary shape: (1, 480, 862)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 862), mask_binary shape: (1, 480, 862)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 1/18\n",
            "mask shape: (1, 480, 862), mask_binary shape: (1, 480, 862)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 862), mask_binary shape: (1, 480, 862)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 2/18\n",
            "mask shape: (1, 480, 862), mask_binary shape: (1, 480, 862)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 862), mask_binary shape: (1, 480, 862)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 3/18\n",
            "mask shape: (1, 480, 862), mask_binary shape: (1, 480, 862)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 862), mask_binary shape: (1, 480, 862)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 4/18\n",
            "mask shape: (1, 480, 862), mask_binary shape: (1, 480, 862)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 862), mask_binary shape: (1, 480, 862)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 5/18\n",
            "mask shape: (1, 480, 862), mask_binary shape: (1, 480, 862)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 862), mask_binary shape: (1, 480, 862)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 6/18\n",
            "mask shape: (1, 480, 862), mask_binary shape: (1, 480, 862)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 862), mask_binary shape: (1, 480, 862)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 7/18\n",
            "mask shape: (1, 480, 862), mask_binary shape: (1, 480, 862)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 862), mask_binary shape: (1, 480, 862)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 8/18\n",
            "mask shape: (1, 480, 862), mask_binary shape: (1, 480, 862)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 862), mask_binary shape: (1, 480, 862)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 9/18\n",
            "mask shape: (1, 480, 862), mask_binary shape: (1, 480, 862)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 862), mask_binary shape: (1, 480, 862)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 10/18\n",
            "mask shape: (1, 480, 862), mask_binary shape: (1, 480, 862)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 862), mask_binary shape: (1, 480, 862)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 11/18\n",
            "mask shape: (1, 480, 862), mask_binary shape: (1, 480, 862)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 862), mask_binary shape: (1, 480, 862)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 12/18\n",
            "mask shape: (1, 480, 862), mask_binary shape: (1, 480, 862)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 862), mask_binary shape: (1, 480, 862)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 13/18\n",
            "mask shape: (1, 480, 862), mask_binary shape: (1, 480, 862)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 862), mask_binary shape: (1, 480, 862)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 14/18\n",
            "mask shape: (1, 480, 862), mask_binary shape: (1, 480, 862)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 862), mask_binary shape: (1, 480, 862)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 15/18\n",
            "mask shape: (1, 480, 862), mask_binary shape: (1, 480, 862)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 862), mask_binary shape: (1, 480, 862)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 16/18\n",
            "mask shape: (1, 480, 862), mask_binary shape: (1, 480, 862)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 862), mask_binary shape: (1, 480, 862)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 17/18\n",
            "mask shape: (1, 480, 862), mask_binary shape: (1, 480, 862)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 862), mask_binary shape: (1, 480, 862)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 18/18\n",
            "Total time taken: 4.16 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "frame loading (JPEG): 100%|██████████| 18/18 [00:00<00:00, 52.31it/s]\n",
            "propagate in video: 100%|██████████| 18/18 [00:00<00:00, 34.22it/s]\n",
            "frame loading (JPEG): 100%|██████████| 18/18 [00:00<00:00, 54.01it/s]\n",
            "propagate in video: 100%|██████████| 18/18 [00:00<00:00, 34.60it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mask shape: (1, 480, 640), mask_binary shape: (1, 480, 640)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 640), mask_binary shape: (1, 480, 640)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 1/18\n",
            "mask shape: (1, 480, 640), mask_binary shape: (1, 480, 640)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 640), mask_binary shape: (1, 480, 640)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 2/18\n",
            "mask shape: (1, 480, 640), mask_binary shape: (1, 480, 640)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 640), mask_binary shape: (1, 480, 640)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 3/18\n",
            "mask shape: (1, 480, 640), mask_binary shape: (1, 480, 640)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 640), mask_binary shape: (1, 480, 640)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 4/18\n",
            "mask shape: (1, 480, 640), mask_binary shape: (1, 480, 640)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 640), mask_binary shape: (1, 480, 640)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 5/18\n",
            "mask shape: (1, 480, 640), mask_binary shape: (1, 480, 640)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 640), mask_binary shape: (1, 480, 640)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 6/18\n",
            "mask shape: (1, 480, 640), mask_binary shape: (1, 480, 640)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 640), mask_binary shape: (1, 480, 640)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 7/18\n",
            "mask shape: (1, 480, 640), mask_binary shape: (1, 480, 640)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 640), mask_binary shape: (1, 480, 640)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 8/18\n",
            "mask shape: (1, 480, 640), mask_binary shape: (1, 480, 640)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 640), mask_binary shape: (1, 480, 640)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 9/18\n",
            "mask shape: (1, 480, 640), mask_binary shape: (1, 480, 640)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 640), mask_binary shape: (1, 480, 640)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 10/18\n",
            "mask shape: (1, 480, 640), mask_binary shape: (1, 480, 640)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 640), mask_binary shape: (1, 480, 640)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 11/18\n",
            "mask shape: (1, 480, 640), mask_binary shape: (1, 480, 640)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 640), mask_binary shape: (1, 480, 640)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 12/18\n",
            "mask shape: (1, 480, 640), mask_binary shape: (1, 480, 640)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 640), mask_binary shape: (1, 480, 640)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 13/18\n",
            "mask shape: (1, 480, 640), mask_binary shape: (1, 480, 640)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 640), mask_binary shape: (1, 480, 640)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 14/18\n",
            "mask shape: (1, 480, 640), mask_binary shape: (1, 480, 640)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 640), mask_binary shape: (1, 480, 640)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 15/18\n",
            "mask shape: (1, 480, 640), mask_binary shape: (1, 480, 640)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 640), mask_binary shape: (1, 480, 640)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 16/18\n",
            "mask shape: (1, 480, 640), mask_binary shape: (1, 480, 640)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 640), mask_binary shape: (1, 480, 640)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 17/18\n",
            "mask shape: (1, 480, 640), mask_binary shape: (1, 480, 640)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 640), mask_binary shape: (1, 480, 640)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 18/18\n",
            "Total time taken: 4.11 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "frame loading (JPEG): 100%|██████████| 18/18 [00:00<00:00, 50.18it/s]\n",
            "propagate in video: 100%|██████████| 18/18 [00:00<00:00, 33.99it/s]\n",
            "frame loading (JPEG): 100%|██████████| 18/18 [00:00<00:00, 52.09it/s]\n",
            "propagate in video: 100%|██████████| 18/18 [00:00<00:00, 34.60it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 1/18\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 2/18\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 3/18\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 4/18\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 5/18\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 6/18\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 7/18\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 8/18\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 9/18\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False]\n",
            "Processed frame 10/18\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 11/18\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 12/18\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 13/18\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 14/18\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 15/18\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 16/18\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 17/18\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 854), mask_binary shape: (1, 480, 854)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 18/18\n",
            "Total time taken: 4.95 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "frame loading (JPEG): 100%|██████████| 18/18 [00:00<00:00, 52.98it/s]\n",
            "propagate in video: 100%|██████████| 18/18 [00:00<00:00, 33.89it/s]\n",
            "frame loading (JPEG): 100%|██████████| 18/18 [00:00<00:00, 53.45it/s]\n",
            "propagate in video: 100%|██████████| 18/18 [00:00<00:00, 34.61it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mask shape: (1, 480, 720), mask_binary shape: (1, 480, 720)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 720), mask_binary shape: (1, 480, 720)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 1/18\n",
            "mask shape: (1, 480, 720), mask_binary shape: (1, 480, 720)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 720), mask_binary shape: (1, 480, 720)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 2/18\n",
            "mask shape: (1, 480, 720), mask_binary shape: (1, 480, 720)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 720), mask_binary shape: (1, 480, 720)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 3/18\n",
            "mask shape: (1, 480, 720), mask_binary shape: (1, 480, 720)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 720), mask_binary shape: (1, 480, 720)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 4/18\n",
            "mask shape: (1, 480, 720), mask_binary shape: (1, 480, 720)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 720), mask_binary shape: (1, 480, 720)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 5/18\n",
            "mask shape: (1, 480, 720), mask_binary shape: (1, 480, 720)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 720), mask_binary shape: (1, 480, 720)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 6/18\n",
            "mask shape: (1, 480, 720), mask_binary shape: (1, 480, 720)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 720), mask_binary shape: (1, 480, 720)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 7/18\n",
            "mask shape: (1, 480, 720), mask_binary shape: (1, 480, 720)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 720), mask_binary shape: (1, 480, 720)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 8/18\n",
            "mask shape: (1, 480, 720), mask_binary shape: (1, 480, 720)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 720), mask_binary shape: (1, 480, 720)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 9/18\n",
            "mask shape: (1, 480, 720), mask_binary shape: (1, 480, 720)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 720), mask_binary shape: (1, 480, 720)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 10/18\n",
            "mask shape: (1, 480, 720), mask_binary shape: (1, 480, 720)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 720), mask_binary shape: (1, 480, 720)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 11/18\n",
            "mask shape: (1, 480, 720), mask_binary shape: (1, 480, 720)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 720), mask_binary shape: (1, 480, 720)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 12/18\n",
            "mask shape: (1, 480, 720), mask_binary shape: (1, 480, 720)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 720), mask_binary shape: (1, 480, 720)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 13/18\n",
            "mask shape: (1, 480, 720), mask_binary shape: (1, 480, 720)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 720), mask_binary shape: (1, 480, 720)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 14/18\n",
            "mask shape: (1, 480, 720), mask_binary shape: (1, 480, 720)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 720), mask_binary shape: (1, 480, 720)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 15/18\n",
            "mask shape: (1, 480, 720), mask_binary shape: (1, 480, 720)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 720), mask_binary shape: (1, 480, 720)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 16/18\n",
            "mask shape: (1, 480, 720), mask_binary shape: (1, 480, 720)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 720), mask_binary shape: (1, 480, 720)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 17/18\n",
            "mask shape: (1, 480, 720), mask_binary shape: (1, 480, 720)\n",
            "mask_binary unique values: [False  True]\n",
            "mask shape: (1, 480, 720), mask_binary shape: (1, 480, 720)\n",
            "mask_binary unique values: [False  True]\n",
            "Processed frame 18/18\n",
            "Total time taken: 5.15 seconds\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from sam2.build_sam import build_sam2_video_predictor\n",
        "import gc\n",
        "import time\n",
        "import csv\n",
        "import json\n",
        "from skimage.measure import find_contours\n",
        "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
        "\n",
        "# Set device for computation\n",
        "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "print(f\"using device: {device}\")\n",
        "\n",
        "if device.type == \"cuda\":\n",
        "    torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()\n",
        "    if torch.cuda.get_device_properties(0).major >= 8:\n",
        "        torch.backends.cuda.matmul.allow_tf32 = True\n",
        "        torch.backends.cudnn.allow_tf32 = True\n",
        "elif device.type == \"mps\":\n",
        "    print(\"\\nSupport for MPS devices is preliminary. SAM 2 is trained with CUDA and might give numerically different outputs and sometimes degraded performance on MPS. See e.g. https://github.com/pytorch/pytorch/issues/84936 for a discussion.\")\n",
        "\n",
        "# Initialize predictor\n",
        "sam2_checkpoint = \"../checkpoints/sam2.1_hiera_large.pt\"\n",
        "model_cfg = \"configs/sam2.1/sam2.1_hiera_l.yaml\"\n",
        "predictor = build_sam2_video_predictor(model_cfg, sam2_checkpoint, device=device)\n",
        "\n",
        "def show_mask(mask, ax, obj_id=None, random_color=False):\n",
        "    color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0) if random_color else np.array([*plt.get_cmap(\"tab10\")(0 if obj_id is None else obj_id)[:3], 0.6])\n",
        "    ax.imshow(mask.reshape(*mask.shape[-2:], 1) * color.reshape(1, 1, -1))\n",
        "\n",
        "# Load coordinates from CSV file\n",
        "coordinates = []\n",
        "with open('ver.csv', 'r') as f:\n",
        "    reader = csv.DictReader(f)\n",
        "    for row in reader:\n",
        "        coordinates.append({\n",
        "            'folder_name': row['folder_name'],\n",
        "            'point': [float(row['hand_point_x']), float(row['hand_point_y'])],\n",
        "            'box': [float(row['box_x1']), float(row['box_y1']), float(row['box_x2']), float(row['box_y2'])],\n",
        "            'frame': int(row['frame'])\n",
        "        })\n",
        "\n",
        "for coord in coordinates:\n",
        "    folder_name = coord['folder_name']\n",
        "    point = np.array(coord['point'], dtype=np.float32)\n",
        "    box = np.array(coord['box'], dtype=np.float32)\n",
        "    frame_index = coord['frame']\n",
        "\n",
        "    frame_dir = f\"./HOIST/valid/JPEGImages/{folder_name}\"\n",
        "    output_dir_box = f\"./TEST/{folder_name}_output_box\"\n",
        "    output_dir_dot = f\"./TEST/{folder_name}_output_dot\"\n",
        "    os.makedirs(output_dir_box, exist_ok=True)\n",
        "    os.makedirs(output_dir_dot, exist_ok=True)\n",
        "\n",
        "    frame_names = sorted([p for p in os.listdir(frame_dir) if os.path.splitext(p)[-1].lower() in [\".jpg\", \".jpeg\"]], key=lambda p: int(os.path.splitext(p)[0].split('_')[-1]))\n",
        "\n",
        "    # Box prompt\n",
        "    inference_state = predictor.init_state(video_path=frame_dir)\n",
        "    predictor.reset_state(inference_state)\n",
        "    ann_frame_idx = frame_index  # the frame index we interact with\n",
        "    ann_obj_id = 1  # give a unique id to each object we interact with (it can be any integers)\n",
        "    _, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(inference_state, frame_idx=ann_frame_idx, obj_id=ann_obj_id, box=box)\n",
        "\n",
        "    video_segments_box = {out_frame_idx: {out_obj_id: (out_mask_logits[i] > 0.0).cpu().numpy() for i, out_obj_id in enumerate(out_obj_ids)} for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(inference_state)}\n",
        "\n",
        "    # Dot prompt\n",
        "    inference_state = predictor.init_state(video_path=frame_dir)\n",
        "    predictor.reset_state(inference_state)\n",
        "    points, labels = point.reshape(1, -1), np.array([1], np.int32)\n",
        "    _, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(inference_state, frame_idx=frame_index, obj_id=1, points=points, labels=labels)\n",
        "\n",
        "    video_segments_dot = {out_frame_idx: {out_obj_id: (out_mask_logits[i] > 0.0).cpu().numpy() for i, out_obj_id in enumerate(out_obj_ids)} for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(inference_state)}\n",
        "\n",
        "    # Start timing\n",
        "    start_time = time.time()\n",
        "\n",
        "    mask_data_box = {}\n",
        "    mask_data_dot = {}\n",
        "\n",
        "    def mask_to_polygon(mask):\n",
        "        # Convert mask to binary\n",
        "        mask_binary = mask.astype(bool)\n",
        "        print(f\"mask shape: {mask.shape}, mask_binary shape: {mask_binary.shape}\")\n",
        "        print(f\"mask_binary unique values: {np.unique(mask_binary)}\")\n",
        "        \n",
        "        # Check if mask is empty\n",
        "        if mask_binary.size == 0 or np.all(mask_binary == 0):\n",
        "            return []\n",
        "        \n",
        "        # Ensure mask is 2D\n",
        "        if mask_binary.ndim > 2:\n",
        "            mask_binary = mask_binary[0]\n",
        "        \n",
        "        # Find contours\n",
        "        contours = find_contours(mask_binary, level=0.5)\n",
        "        # Convert contours to polygons\n",
        "        polygons = [contour.tolist() for contour in contours]\n",
        "        return polygons\n",
        "\n",
        "    for frame_idx in range(len(frame_names)):\n",
        "        frame_masks_box = []\n",
        "        frame_masks_dot = []\n",
        "        frame = Image.open(os.path.join(frame_dir, frame_names[frame_idx]))\n",
        "        \n",
        "        # Box prompt\n",
        "        for obj_id, mask in video_segments_box.get(frame_idx, {}).items():\n",
        "            polygons = mask_to_polygon(mask)\n",
        "            frame_masks_box.append({\n",
        "                \"obj_id\": obj_id,\n",
        "                \"polygons\": polygons\n",
        "            })\n",
        "        mask_data_box[frame_idx] = frame_masks_box\n",
        "        \n",
        "        # Dot prompt\n",
        "        for obj_id, mask in video_segments_dot.get(frame_idx, {}).items():\n",
        "            polygons = mask_to_polygon(mask)\n",
        "            frame_masks_dot.append({\n",
        "                \"obj_id\": obj_id,\n",
        "                \"polygons\": polygons\n",
        "            })\n",
        "        mask_data_dot[frame_idx] = frame_masks_dot\n",
        "        \n",
        "        # Save images\n",
        "        fig, ax = plt.subplots(figsize=(frame.width / 100, frame.height / 100), dpi=100)\n",
        "        ax.axis('off')\n",
        "        ax.imshow(frame)\n",
        "        \n",
        "        # Box prompt mask\n",
        "        for obj_id, mask in video_segments_box.get(frame_idx, {}).items():\n",
        "            show_mask(mask, ax, obj_id=obj_id)\n",
        "        canvas = FigureCanvas(fig)\n",
        "        canvas.draw()\n",
        "        img = np.frombuffer(canvas.tostring_argb(), dtype='uint8').reshape(fig.canvas.get_width_height()[::-1] + (4,))\n",
        "        img = img[:, :, [1, 2, 3]]  # Convert ARGB to RGB\n",
        "        output_frame_path = os.path.join(output_dir_box, f\"{frame_idx:05d}.jpg\")\n",
        "        cv2.imwrite(output_frame_path, cv2.cvtColor(img, cv2.COLOR_RGB2BGR))\n",
        "        plt.close(fig)\n",
        "        \n",
        "        # Dot prompt mask\n",
        "        fig, ax = plt.subplots(figsize=(frame.width / 100, frame.height / 100), dpi=100)\n",
        "        ax.axis('off')\n",
        "        ax.imshow(frame)\n",
        "        for obj_id, mask in video_segments_dot.get(frame_idx, {}).items():\n",
        "            show_mask(mask, ax, obj_id=obj_id)\n",
        "        canvas = FigureCanvas(fig)\n",
        "        canvas.draw()\n",
        "        img = np.frombuffer(canvas.tostring_argb(), dtype='uint8').reshape(fig.canvas.get_width_height()[::-1] + (4,))\n",
        "        img = img[:, :, [1, 2, 3]]  # Convert ARGB to RGB\n",
        "        output_frame_path = os.path.join(output_dir_dot, f\"{frame_idx:05d}.jpg\")\n",
        "        cv2.imwrite(output_frame_path, cv2.cvtColor(img, cv2.COLOR_RGB2BGR))\n",
        "        plt.close(fig)\n",
        "        \n",
        "        del frame, fig, ax, canvas, img  # Free up memory\n",
        "        gc.collect()  # Force garbage collection\n",
        "        print(f\"Processed frame {frame_idx + 1}/{len(frame_names)}\")\n",
        "\n",
        "    # Save mask data to JSON\n",
        "    with open(os.path.join(output_dir_box, \"mask_data_box.json\"), \"w\") as json_file:\n",
        "        json.dump(mask_data_box, json_file, indent=4)\n",
        "\n",
        "    with open(os.path.join(output_dir_dot, \"mask_data_dot.json\"), \"w\") as json_file:\n",
        "        json.dump(mask_data_dot, json_file, indent=4)\n",
        "\n",
        "    # End timing\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"Total time taken: {elapsed_time:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import csv\n",
        "\n",
        "# Load coordinates from CSV file\n",
        "coordinates = []\n",
        "with open('final.csv', 'r') as f:\n",
        "    reader = csv.DictReader(f)\n",
        "    for row in reader:\n",
        "        coordinates.append({\n",
        "            'folder_name': row['folder_name'],\n",
        "            'box': [float(row['box_x1']), float(row['box_y1']), float(row['box_x1']) + float(row['width']), float(row['box_y1']) + float(row['height'])],\n",
        "            'frame': int(row['frame']),\n",
        "            'point_x': float(row['point_x']) if row['point_x'] else None,\n",
        "            'point_y': float(row['point_y']) if row['point_y'] else None\n",
        "        })\n",
        "\n",
        "for coord in coordinates:\n",
        "    folder_name = coord['folder_name']\n",
        "    box = np.array(coord['box'], dtype=np.float32)\n",
        "    frame_index = coord['frame']\n",
        "    point_x = coord['point_x']\n",
        "    point_y = coord['point_y']\n",
        "\n",
        "    frame_dir = f\"./HOIST/valid/JPEGImages/{folder_name}\"\n",
        "    output_dir = f\"./TEST/ver2\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    frame_names = sorted([p for p in os.listdir(frame_dir) if os.path.splitext(p)[-1].lower() in [\".jpg\", \".jpeg\"]], key=lambda p: int(os.path.splitext(p)[0].split('_')[-1]))\n",
        "\n",
        "    if frame_names and frame_index < len(frame_names):\n",
        "        frame_path = os.path.join(frame_dir, frame_names[frame_index])\n",
        "        frame = cv2.imread(frame_path)\n",
        "\n",
        "        # Draw the box on the specified frame\n",
        "        x1, y1, x2, y2 = map(int, box)\n",
        "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "\n",
        "        # Draw the center point on the specified frame\n",
        "        if point_x is not None and point_y is not None:\n",
        "            cv2.circle(frame, (int(point_x), int(point_y)), 5, (0, 0, 255), -1)\n",
        "\n",
        "        # Save the image with the box and point\n",
        "        output_frame_path = os.path.join(output_dir, f\"{folder_name}_frame_{frame_index}_with_box_and_point.jpg\")\n",
        "        cv2.imwrite(output_frame_path, frame)\n",
        "\n",
        "        print(f\"Processed folder {folder_name}, frame {frame_index}\")\n",
        "\n",
        "print(\"Verification complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from PIL import Image\n",
        "import gc\n",
        "import time\n",
        "import csv\n",
        "import json\n",
        "from skimage.measure import find_contours\n",
        "from sam2.build_sam import build_sam2_video_predictor\n",
        "\n",
        "# Set device for computation\n",
        "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "print(f\"using device: {device}\")\n",
        "\n",
        "if device.type == \"cuda\":\n",
        "    torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()\n",
        "    if torch.cuda.get_device_properties(0).major >= 8:\n",
        "        torch.backends.cuda.matmul.allow_tf32 = True\n",
        "        torch.backends.cudnn.allow_tf32 = True\n",
        "elif device.type == \"mps\":\n",
        "    print(\"\\nSupport for MPS devices is preliminary. SAM 2 is trained with CUDA and might give numerically different outputs and sometimes degraded performance on MPS. See e.g. https://github.com/pytorch/pytorch/issues/84936 for a discussion.\")\n",
        "\n",
        "# Initialize predictor\n",
        "sam2_checkpoint = \"../checkpoints/sam2.1_hiera_large.pt\"\n",
        "model_cfg = \"configs/sam2.1/sam2.1_hiera_l.yaml\"\n",
        "predictor = build_sam2_video_predictor(model_cfg, sam2_checkpoint, device=device)\n",
        "\n",
        "# Load coordinates from CSV file\n",
        "coordinates = []\n",
        "with open('ver.csv', 'r') as f:\n",
        "    reader = csv.DictReader(f)\n",
        "    for row in reader:\n",
        "        coordinates.append({\n",
        "            'folder_name': row['folder_name'],\n",
        "            'point': [float(row['hand_point_x']), float(row['hand_point_y'])],\n",
        "            'box': [float(row['box_x1']), float(row['box_y1']), float(row['box_x2']) , float(row['box_y1'])],\n",
        "            'frame': int(row['frame'])\n",
        "        })\n",
        "\n",
        "for coord in coordinates:\n",
        "    folder_name = coord['folder_name']\n",
        "    point = np.array(coord['point'], dtype=np.float32)\n",
        "    box = np.array(coord['box'], dtype=np.float32)\n",
        "    frame_index = coord['frame']\n",
        "\n",
        "    frame_dir = f\"./HOIST/valid/JPEGImages/{folder_name}\"\n",
        "    output_dir_box = f\"./collected/box\"\n",
        "    output_dir_dot = f\"./collected/dot\"\n",
        "    os.makedirs(output_dir_box, exist_ok=True)\n",
        "    os.makedirs(output_dir_dot, exist_ok=True)\n",
        "\n",
        "    frame_names = sorted([p for p in os.listdir(frame_dir) if os.path.splitext(p)[-1].lower() in [\".jpg\", \".jpeg\"]], key=lambda p: int(os.path.splitext(p)[0].split('_')[-1]))\n",
        "\n",
        "    # Box prompt\n",
        "    inference_state = predictor.init_state(video_path=frame_dir)\n",
        "    predictor.reset_state(inference_state)\n",
        "    ann_frame_idx = frame_index  # the frame index we interact with\n",
        "    ann_obj_id = 1  # give a unique id to each object we interact with (it can be any integers)\n",
        "    _, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(inference_state, frame_idx=ann_frame_idx, obj_id=ann_obj_id, box=box)\n",
        "\n",
        "    video_segments_box = {out_frame_idx: {out_obj_id: (out_mask_logits[i] > 0.0).cpu().numpy() for i, out_obj_id in enumerate(out_obj_ids)} for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(inference_state)}\n",
        "\n",
        "    # Dot prompt\n",
        "    inference_state = predictor.init_state(video_path=frame_dir)\n",
        "    predictor.reset_state(inference_state)\n",
        "    points, labels = point.reshape(1, -1), np.array([1], np.int32)\n",
        "    _, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(inference_state, frame_idx=frame_index, obj_id=1, points=points, labels=labels)\n",
        "\n",
        "    video_segments_dot = {out_frame_idx: {out_obj_id: (out_mask_logits[i] > 0.0).cpu().numpy() for i, out_obj_id in enumerate(out_obj_ids)} for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(inference_state)}\n",
        "\n",
        "    # Start timing\n",
        "    start_time = time.time()\n",
        "\n",
        "    mask_data_box = {}\n",
        "    mask_data_dot = {}\n",
        "\n",
        "    def mask_to_polygon(mask):\n",
        "        # Convert mask to binary\n",
        "        mask_binary = mask.astype(bool)\n",
        "        print(f\"mask shape: {mask.shape}, mask_binary shape: {mask_binary.shape}\")\n",
        "        print(f\"mask_binary unique values: {np.unique(mask_binary)}\")\n",
        "        \n",
        "        # Check if mask is empty\n",
        "        if mask_binary.size == 0 or np.all(mask_binary == 0):\n",
        "            return []\n",
        "        \n",
        "        # Ensure mask is 2D\n",
        "        if mask_binary.ndim > 2:\n",
        "            mask_binary = mask_binary[0]\n",
        "        \n",
        "        # Find contours\n",
        "        contours = find_contours(mask_binary, level=0.5)\n",
        "        # Convert contours to polygons\n",
        "        polygons = [contour.tolist() for contour in contours]\n",
        "        return polygons\n",
        "\n",
        "    for frame_idx in range(len(frame_names)):\n",
        "        frame_masks_box = []\n",
        "        frame_masks_dot = []\n",
        "        \n",
        "        # Box prompt\n",
        "        for obj_id, mask in video_segments_box.get(frame_idx, {}).items():\n",
        "            polygons = mask_to_polygon(mask)\n",
        "            frame_masks_box.append({\n",
        "                \"obj_id\": obj_id,\n",
        "                \"polygons\": polygons\n",
        "            })\n",
        "        mask_data_box[frame_idx] = frame_masks_box\n",
        "        \n",
        "        # Dot prompt\n",
        "        for obj_id, mask in video_segments_dot.get(frame_idx, {}).items():\n",
        "            polygons = mask_to_polygon(mask)\n",
        "            frame_masks_dot.append({\n",
        "                \"obj_id\": obj_id,\n",
        "                \"polygons\": polygons\n",
        "            })\n",
        "        mask_data_dot[frame_idx] = frame_masks_dot\n",
        "        \n",
        "        gc.collect()  # Force garbage collection\n",
        "        print(f\"Processed frame {frame_idx + 1}/{len(frame_names)}\")\n",
        "\n",
        "    # Save mask data to JSON\n",
        "    with open(os.path.join(output_dir_box, f\"{folder_name}.json\"), \"w\") as json_file:\n",
        "        json.dump(mask_data_box, json_file, indent=4)\n",
        "\n",
        "    with open(os.path.join(output_dir_dot, f\"{folder_name}.json\"), \"w\") as json_file:\n",
        "        json.dump(mask_data_dot, json_file, indent=4)\n",
        "\n",
        "    # End timing\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"Total time taken: {elapsed_time:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For verifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed folder GQ9DjpYb8kI_453, frame 0\n",
            "Verification complete.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import csv\n",
        "\n",
        "# Load coordinates from CSV file\n",
        "coordinates = []\n",
        "with open('/home/totargaming/workspace/sam2/aria-handtracking/ver.csv', 'r') as f:\n",
        "    reader = csv.DictReader(f)\n",
        "    for row in reader:\n",
        "        coordinates.append({\n",
        "            'folder_name': row['folder_name'],\n",
        "            'hand_point_x': float(row['hand_point_x']),\n",
        "            'hand_point_y': float(row['hand_point_y']),\n",
        "            'obj_point_x': float(row['obj_point_x']),\n",
        "            'obj_point_y': float(row['obj_point_y']),\n",
        "            'box_x1': float(row['box_x1']),\n",
        "            'box_y1': float(row['box_y1']),\n",
        "            'box_x2': float(row['box_x2']),\n",
        "            'box_y2': float(row['box_y2']),\n",
        "            'frame': int(row['frame'])\n",
        "        })\n",
        "\n",
        "for coord in coordinates:\n",
        "    folder_name = coord['folder_name']\n",
        "    hand_point_x = coord['hand_point_x']\n",
        "    hand_point_y = coord['hand_point_y']\n",
        "    obj_point_x = coord['obj_point_x']\n",
        "    obj_point_y = coord['obj_point_y']\n",
        "    box_x1 = coord['box_x1']\n",
        "    box_y1 = coord['box_y1']\n",
        "    box_x2 = coord['box_x2']\n",
        "    box_y2 = coord['box_y2']\n",
        "    frame_index = coord['frame']\n",
        "\n",
        "    frame_dir = f\"./HOIST/valid/JPEGImages/{folder_name}\"\n",
        "    output_dir = f\"./TEST2/verifier\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    frame_names = sorted([p for p in os.listdir(frame_dir) if os.path.splitext(p)[-1].lower() in [\".jpg\", \".jpeg\"]], key=lambda p: int(os.path.splitext(p)[0].split('_')[-1]))\n",
        "\n",
        "    if frame_names and frame_index < len(frame_names):\n",
        "        frame_path = os.path.join(frame_dir, frame_names[frame_index])\n",
        "        frame = cv2.imread(frame_path)\n",
        "\n",
        "        # Draw the combined bounding box on the specified frame\n",
        "        cv2.rectangle(frame, (int(box_x1), int(box_y1)), (int(box_x2), int(box_y2)), (0, 255, 0), 2)\n",
        "\n",
        "        # Draw the hand point on the specified frame\n",
        "        cv2.circle(frame, (int(hand_point_x), int(hand_point_y)), 5, (0, 0, 255), -1)\n",
        "\n",
        "        # Draw the object point on the specified frame\n",
        "        cv2.circle(frame, (int(obj_point_x), int(obj_point_y)), 5, (255, 0, 0), -1)\n",
        "\n",
        "        # Save the image with the box and points\n",
        "        output_frame_path = os.path.join(output_dir, f\"{folder_name}_frame_{frame_index}_with_box_and_points.jpg\")\n",
        "        cv2.imwrite(output_frame_path, frame)\n",
        "\n",
        "        print(f\"Processed folder {folder_name}, frame {frame_index}\")\n",
        "\n",
        "print(\"Verification complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "using device: cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "frame loading (JPEG): 100%|██████████| 18/18 [00:00<00:00, 52.49it/s]\n",
            "propagate in video: 100%|██████████| 18/18 [00:00<00:00, 33.40it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed frame 1/18\n",
            "Processed frame 2/18\n",
            "Processed frame 3/18\n",
            "Processed frame 4/18\n",
            "Processed frame 5/18\n",
            "Processed frame 6/18\n",
            "Processed frame 7/18\n",
            "Processed frame 8/18\n",
            "Processed frame 9/18\n",
            "Processed frame 10/18\n",
            "Processed frame 11/18\n",
            "Processed frame 12/18\n",
            "Processed frame 13/18\n",
            "Processed frame 14/18\n",
            "Processed frame 15/18\n",
            "Processed frame 16/18\n",
            "Processed frame 17/18\n",
            "Processed frame 18/18\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'start_time' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 120\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmask_data.json\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m json_file:\n\u001b[1;32m    118\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(mask_data, json_file, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m--> 120\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal time taken: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[43mstart_time\u001b[49m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'start_time' is not defined"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from sam2.build_sam import build_sam2_video_predictor\n",
        "import gc\n",
        "import time\n",
        "import csv\n",
        "import json\n",
        "from skimage.measure import find_contours\n",
        "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
        "\n",
        "# Set device for computation\n",
        "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "print(f\"using device: {device}\")\n",
        "\n",
        "if device.type == \"cuda\":\n",
        "    torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()\n",
        "    if torch.cuda.get_device_properties(0).major >= 8:\n",
        "        torch.backends.cuda.matmul.allow_tf32 = True\n",
        "        torch.backends.cudnn.allow_tf32 = True\n",
        "elif device.type == \"mps\":\n",
        "    print(\"\\nSupport for MPS devices is preliminary. SAM 2 is trained with CUDA and might give numerically different outputs and sometimes degraded performance on MPS. See e.g. https://github.com/pytorch/pytorch/issues/84936 for a discussion.\")\n",
        "\n",
        "# Initialize predictor\n",
        "sam2_checkpoint = \"../checkpoints/sam2.1_hiera_large.pt\"\n",
        "model_cfg = \"configs/sam2.1/sam2.1_hiera_l.yaml\"\n",
        "predictor = build_sam2_video_predictor(model_cfg, sam2_checkpoint, device=device)\n",
        "\n",
        "def show_mask(mask, ax, obj_id=None, random_color=False):\n",
        "    color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0) if random_color else np.array([*plt.get_cmap(\"tab10\")(0 if obj_id is None else obj_id)[:3], 0.6])\n",
        "    ax.imshow(mask.reshape(*mask.shape[-2:], 1) * color.reshape(1, 1, -1))\n",
        "\n",
        "# Load coordinates from CSV file\n",
        "coordinates = []\n",
        "with open('/home/totargaming/workspace/sam2/aria-handtracking/ver.csv', 'r') as f:\n",
        "    reader = csv.DictReader(f)\n",
        "    for row in reader:\n",
        "        coordinates.append({\n",
        "            'folder_name': row['folder_name'],\n",
        "            'hand_point_x': float(row['hand_point_x']),\n",
        "            'hand_point_y': float(row['hand_point_y']),\n",
        "            'obj_point_x': float(row['obj_point_x']),\n",
        "            'obj_point_y': float(row['obj_point_y']),\n",
        "            'box_x1': float(row['box_x1']),\n",
        "            'box_y1': float(row['box_y1']),\n",
        "            'box_x2': float(row['box_x2']),\n",
        "            'box_y2': float(row['box_y2']),\n",
        "            'frame': int(row['frame'])\n",
        "        })\n",
        "\n",
        "for coord in coordinates:\n",
        "    folder_name = coord['folder_name']\n",
        "    hand_point_x = coord['hand_point_x']\n",
        "    hand_point_y = coord['hand_point_y']\n",
        "    obj_point_x = coord['obj_point_x']\n",
        "    obj_point_y = coord['obj_point_y']\n",
        "    box_x1 = coord['box_x1']\n",
        "    box_y1 = coord['box_y1']\n",
        "    box_x2 = coord['box_x2']\n",
        "    box_y2 = coord['box_y2']\n",
        "    frame_index = coord['frame']\n",
        "\n",
        "    frame_dir = f\"./HOIST/valid/JPEGImages/{folder_name}\"\n",
        "    output_dir = f\"./TEST2/{folder_name}_output\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    frame_names = sorted([p for p in os.listdir(frame_dir) if os.path.splitext(p)[-1].lower() in [\".jpg\", \".jpeg\"]], key=lambda p: int(os.path.splitext(p)[0].split('_')[-1]))\n",
        "\n",
        "    if frame_names and frame_index < len(frame_names):\n",
        "        frame_path = os.path.join(frame_dir, frame_names[frame_index])\n",
        "        frame = cv2.imread(frame_path)\n",
        "\n",
        "        # Initialize inference state\n",
        "        inference_state = predictor.init_state(video_path=frame_dir)\n",
        "        predictor.reset_state(inference_state)\n",
        "\n",
        "        # Add bounding box for the object\n",
        "        obj_box = np.array([box_x1, box_y1, box_x2, box_y2], dtype=np.float32)\n",
        "        _, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(inference_state, frame_idx=frame_index, obj_id=1, box=obj_box)\n",
        "\n",
        "        # Add dot for the hand as a negative prompt\n",
        "        points = np.array([[hand_point_x, hand_point_y]], dtype=np.float32)\n",
        "        labels = np.array([0], np.int32)  # 0 indicates a negative prompt\n",
        "        _, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(inference_state, frame_idx=frame_index, obj_id=1, points=points, labels=labels)\n",
        "\n",
        "        # Propagate the prompts to get the masklet across the video\n",
        "        video_segments = {out_frame_idx: {out_obj_id: (out_mask_logits[i] > 0.0).cpu().numpy() for i, out_obj_id in enumerate(out_obj_ids)} for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(inference_state)}\n",
        "\n",
        "        # Save images\n",
        "        for frame_idx in range(len(frame_names)):\n",
        "            frame = Image.open(os.path.join(frame_dir, frame_names[frame_idx]))\n",
        "            fig, ax = plt.subplots(figsize=(frame.width / 100, frame.height / 100), dpi=100)\n",
        "            ax.axis('off')\n",
        "            ax.imshow(frame)\n",
        "\n",
        "            for obj_id, mask in video_segments.get(frame_idx, {}).items():\n",
        "                show_mask(mask, ax, obj_id=obj_id)\n",
        "\n",
        "            canvas = FigureCanvas(fig)\n",
        "            canvas.draw()\n",
        "            img = np.frombuffer(canvas.tostring_argb(), dtype='uint8').reshape(fig.canvas.get_width_height()[::-1] + (4,))\n",
        "            img = img[:, :, [1, 2, 3]]  # Convert ARGB to RGB\n",
        "            output_frame_path = os.path.join(output_dir, f\"{frame_idx:05d}.jpg\")\n",
        "            cv2.imwrite(output_frame_path, cv2.cvtColor(img, cv2.COLOR_RGB2BGR))\n",
        "            plt.close(fig)\n",
        "\n",
        "            del frame, fig, ax, canvas, img  # Free up memory\n",
        "            gc.collect()  # Force garbage collection\n",
        "            print(f\"Processed frame {frame_idx + 1}/{len(frame_names)}\")\n",
        "\n",
        "        # Save mask data to JSON\n",
        "        mask_data = {frame_idx: {obj_id: mask.tolist() for obj_id, mask in masks.items()} for frame_idx, masks in video_segments.items()}\n",
        "        with open(os.path.join(output_dir, \"mask_data.json\"), \"w\") as json_file:\n",
        "            json.dump(mask_data, json_file, indent=4)\n",
        "\n",
        "        print(f\"Total time taken: {time.time() - start_time:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "using device: cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "frame loading (JPEG): 100%|██████████| 18/18 [00:00<00:00, 49.19it/s]\n",
            "propagate in video: 100%|██████████| 18/18 [00:00<00:00, 33.82it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed frame 1/18\n",
            "Processed frame 2/18\n",
            "Processed frame 3/18\n",
            "Processed frame 4/18\n",
            "Processed frame 5/18\n",
            "Processed frame 6/18\n",
            "Processed frame 7/18\n",
            "Processed frame 8/18\n",
            "Processed frame 9/18\n",
            "Processed frame 10/18\n",
            "Processed frame 11/18\n",
            "Processed frame 12/18\n",
            "Processed frame 13/18\n",
            "Processed frame 14/18\n",
            "Processed frame 15/18\n",
            "Processed frame 16/18\n",
            "Processed frame 17/18\n",
            "Processed frame 18/18\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "frame loading (JPEG): 100%|██████████| 67/67 [00:01<00:00, 48.35it/s]\n",
            "propagate in video: 100%|██████████| 67/67 [00:02<00:00, 32.19it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed frame 1/67\n",
            "Processed frame 2/67\n",
            "Processed frame 3/67\n",
            "Processed frame 4/67\n",
            "Processed frame 5/67\n",
            "Processed frame 6/67\n",
            "Processed frame 7/67\n",
            "Processed frame 8/67\n",
            "Processed frame 9/67\n",
            "Processed frame 10/67\n",
            "Processed frame 11/67\n",
            "Processed frame 12/67\n",
            "Processed frame 13/67\n",
            "Processed frame 14/67\n",
            "Processed frame 15/67\n",
            "Processed frame 16/67\n",
            "Processed frame 17/67\n",
            "Processed frame 18/67\n",
            "Processed frame 19/67\n",
            "Processed frame 20/67\n",
            "Processed frame 21/67\n",
            "Processed frame 22/67\n",
            "Processed frame 23/67\n",
            "Processed frame 24/67\n",
            "Processed frame 25/67\n",
            "Processed frame 26/67\n",
            "Processed frame 27/67\n",
            "Processed frame 28/67\n",
            "Processed frame 29/67\n",
            "Processed frame 30/67\n",
            "Processed frame 31/67\n",
            "Processed frame 32/67\n",
            "Processed frame 33/67\n",
            "Processed frame 34/67\n",
            "Processed frame 35/67\n",
            "Processed frame 36/67\n",
            "Processed frame 37/67\n",
            "Processed frame 38/67\n",
            "Processed frame 39/67\n",
            "Processed frame 40/67\n",
            "Processed frame 41/67\n",
            "Processed frame 42/67\n",
            "Processed frame 43/67\n",
            "Processed frame 44/67\n",
            "Processed frame 45/67\n",
            "Processed frame 46/67\n",
            "Processed frame 47/67\n",
            "Processed frame 48/67\n",
            "Processed frame 49/67\n",
            "Processed frame 50/67\n",
            "Processed frame 51/67\n",
            "Processed frame 52/67\n",
            "Processed frame 53/67\n",
            "Processed frame 54/67\n",
            "Processed frame 55/67\n",
            "Processed frame 56/67\n",
            "Processed frame 57/67\n",
            "Processed frame 58/67\n",
            "Processed frame 59/67\n",
            "Processed frame 60/67\n",
            "Processed frame 61/67\n",
            "Processed frame 62/67\n",
            "Processed frame 63/67\n",
            "Processed frame 64/67\n",
            "Processed frame 65/67\n",
            "Processed frame 66/67\n",
            "Processed frame 67/67\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "frame loading (JPEG): 100%|██████████| 18/18 [00:00<00:00, 49.47it/s]\n",
            "propagate in video: 100%|██████████| 18/18 [00:00<00:00, 33.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed frame 1/18\n",
            "Processed frame 2/18\n",
            "Processed frame 3/18\n",
            "Processed frame 4/18\n",
            "Processed frame 5/18\n",
            "Processed frame 6/18\n",
            "Processed frame 7/18\n",
            "Processed frame 8/18\n",
            "Processed frame 9/18\n",
            "Processed frame 10/18\n",
            "Processed frame 11/18\n",
            "Processed frame 12/18\n",
            "Processed frame 13/18\n",
            "Processed frame 14/18\n",
            "Processed frame 15/18\n",
            "Processed frame 16/18\n",
            "Processed frame 17/18\n",
            "Processed frame 18/18\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "frame loading (JPEG): 100%|██████████| 18/18 [00:00<00:00, 49.37it/s]\n",
            "propagate in video: 100%|██████████| 18/18 [00:00<00:00, 33.72it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed frame 1/18\n",
            "Processed frame 2/18\n",
            "Processed frame 3/18\n",
            "Processed frame 4/18\n",
            "Processed frame 5/18\n",
            "Processed frame 6/18\n",
            "Processed frame 7/18\n",
            "Processed frame 8/18\n",
            "Processed frame 9/18\n",
            "Processed frame 10/18\n",
            "Processed frame 11/18\n",
            "Processed frame 12/18\n",
            "Processed frame 13/18\n",
            "Processed frame 14/18\n",
            "Processed frame 15/18\n",
            "Processed frame 16/18\n",
            "Processed frame 17/18\n",
            "Processed frame 18/18\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "frame loading (JPEG): 100%|██████████| 18/18 [00:00<00:00, 50.16it/s]\n",
            "propagate in video: 100%|██████████| 18/18 [00:00<00:00, 33.35it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed frame 1/18\n",
            "Processed frame 2/18\n",
            "Processed frame 3/18\n",
            "Processed frame 4/18\n",
            "Processed frame 5/18\n",
            "Processed frame 6/18\n",
            "Processed frame 7/18\n",
            "Processed frame 8/18\n",
            "Processed frame 9/18\n",
            "Processed frame 10/18\n",
            "Processed frame 11/18\n",
            "Processed frame 12/18\n",
            "Processed frame 13/18\n",
            "Processed frame 14/18\n",
            "Processed frame 15/18\n",
            "Processed frame 16/18\n",
            "Processed frame 17/18\n",
            "Processed frame 18/18\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from sam2.build_sam import build_sam2_video_predictor\n",
        "import gc\n",
        "import time\n",
        "import csv\n",
        "import json\n",
        "from skimage.measure import find_contours\n",
        "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
        "\n",
        "# Set device for computation\n",
        "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "print(f\"using device: {device}\")\n",
        "\n",
        "if device.type == \"cuda\":\n",
        "    torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()\n",
        "    if torch.cuda.get_device_properties(0).major >= 8:\n",
        "        torch.backends.cuda.matmul.allow_tf32 = True\n",
        "        torch.backends.cudnn.allow_tf32 = True\n",
        "elif device.type == \"mps\":\n",
        "    print(\"\\nSupport for MPS devices is preliminary. SAM 2 is trained with CUDA and might give numerically different outputs and sometimes degraded performance on MPS. See e.g. https://github.com/pytorch/pytorch/issues/84936 for a discussion.\")\n",
        "\n",
        "# Initialize predictor\n",
        "sam2_checkpoint = \"../checkpoints/sam2.1_hiera_large.pt\"\n",
        "model_cfg = \"configs/sam2.1/sam2.1_hiera_l.yaml\"\n",
        "predictor = build_sam2_video_predictor(model_cfg, sam2_checkpoint, device=device)\n",
        "\n",
        "def show_mask(mask, ax, obj_id=None, random_color=False):\n",
        "    color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0) if random_color else np.array([*plt.get_cmap(\"tab10\")(0 if obj_id is None else obj_id)[:3], 0.6])\n",
        "    ax.imshow(mask.reshape(*mask.shape[-2:], 1) * color.reshape(1, 1, -1))\n",
        "\n",
        "# Load coordinates from CSV file\n",
        "coordinates = []\n",
        "with open('/home/totargaming/workspace/sam2/aria-handtracking/ver.csv', 'r') as f:\n",
        "    reader = csv.DictReader(f)\n",
        "    for row in reader:\n",
        "        coordinates.append({\n",
        "            'folder_name': row['folder_name'],\n",
        "            'hand_point_x': float(row['hand_point_x']),\n",
        "            'hand_point_y': float(row['hand_point_y']),\n",
        "            'obj_point_x': float(row['obj_point_x']),\n",
        "            'obj_point_y': float(row['obj_point_y']),\n",
        "            'box_x1': float(row['box_x1']),\n",
        "            'box_y1': float(row['box_y1']),\n",
        "            'box_x2': float(row['box_x2']),\n",
        "            'box_y2': float(row['box_y2']),\n",
        "            'frame': int(row['frame'])\n",
        "        })\n",
        "\n",
        "for coord in coordinates:\n",
        "    folder_name = coord['folder_name']\n",
        "    hand_point_x = coord['hand_point_x']\n",
        "    hand_point_y = coord['hand_point_y']\n",
        "    obj_point_x = coord['obj_point_x']\n",
        "    obj_point_y = coord['obj_point_y']\n",
        "    box_x1 = coord['box_x1']\n",
        "    box_y1 = coord['box_y1']\n",
        "    box_x2 = coord['box_x2']\n",
        "    box_y2 = coord['box_y2']\n",
        "    frame_index = coord['frame']\n",
        "\n",
        "    frame_dir = f\"./HOIST/valid/JPEGImages/{folder_name}\"\n",
        "    output_dir = f\"./TEST2/{folder_name}_output_dot\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    frame_names = sorted([p for p in os.listdir(frame_dir) if os.path.splitext(p)[-1].lower() in [\".jpg\", \".jpeg\"]], key=lambda p: int(os.path.splitext(p)[0].split('_')[-1]))\n",
        "\n",
        "    if frame_names and frame_index < len(frame_names):\n",
        "        frame_path = os.path.join(frame_dir, frame_names[frame_index])\n",
        "        frame = cv2.imread(frame_path)\n",
        "\n",
        "        # Initialize inference state\n",
        "        inference_state = predictor.init_state(video_path=frame_dir)\n",
        "        predictor.reset_state(inference_state)\n",
        "\n",
        "        # Add positive prompts for the hand and the object\n",
        "        points = np.array([[hand_point_x, hand_point_y], [obj_point_x, obj_point_y]], dtype=np.float32)\n",
        "        labels = np.array([1, 1], np.int32)  # 1 indicates a positive prompt\n",
        "        _, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(inference_state, frame_idx=frame_index, obj_id=1, points=points, labels=labels)\n",
        "\n",
        "        # Add the hand point as a negative prompt\n",
        "        points = np.array([[hand_point_x, hand_point_y]], dtype=np.float32)\n",
        "        labels = np.array([0], np.int32)  # 0 indicates a negative prompt\n",
        "        _, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(inference_state, frame_idx=frame_index, obj_id=1, points=points, labels=labels)\n",
        "\n",
        "        # Propagate the prompts to get the masklet across the video\n",
        "        video_segments = {out_frame_idx: {out_obj_id: (out_mask_logits[i] > 0.0).cpu().numpy() for i, out_obj_id in enumerate(out_obj_ids)} for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(inference_state)}\n",
        "\n",
        "        # Save images\n",
        "        for frame_idx in range(len(frame_names)):\n",
        "            frame = Image.open(os.path.join(frame_dir, frame_names[frame_idx]))\n",
        "            fig, ax = plt.subplots(figsize=(frame.width / 100, frame.height / 100), dpi=100)\n",
        "            ax.axis('off')\n",
        "            ax.imshow(frame)\n",
        "\n",
        "            for obj_id, mask in video_segments.get(frame_idx, {}).items():\n",
        "                show_mask(mask, ax, obj_id=obj_id)\n",
        "\n",
        "            canvas = FigureCanvas(fig)\n",
        "            canvas.draw()\n",
        "            img = np.frombuffer(canvas.tostring_argb(), dtype='uint8').reshape(fig.canvas.get_width_height()[::-1] + (4,))\n",
        "            img = img[:, :, [1, 2, 3]]  # Convert ARGB to RGB\n",
        "            output_frame_path = os.path.join(output_dir, f\"{frame_idx:05d}.jpg\")\n",
        "            cv2.imwrite(output_frame_path, cv2.cvtColor(img, cv2.COLOR_RGB2BGR))\n",
        "            plt.close(fig)\n",
        "\n",
        "            del frame, fig, ax, canvas, img  # Free up memory\n",
        "            gc.collect()  # Force garbage collection\n",
        "            print(f\"Processed frame {frame_idx + 1}/{len(frame_names)}\")\n",
        "\n",
        "        # Save mask data to JSON\n",
        "        mask_data = {frame_idx: {obj_id: mask.tolist() for obj_id, mask in masks.items()} for frame_idx, masks in video_segments.items()}\n",
        "        with open(os.path.join(output_dir, \"mask_data.json\"), \"w\") as json_file:\n",
        "            json.dump(mask_data, json_file, indent=4)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from PIL import Image\n",
        "import gc\n",
        "import time\n",
        "import csv\n",
        "import json\n",
        "from skimage.measure import find_contours\n",
        "from sam2.build_sam import build_sam2_video_predictor\n",
        "\n",
        "# Set device for computation\n",
        "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "print(f\"using device: {device}\")\n",
        "\n",
        "if device.type == \"cuda\":\n",
        "    torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()\n",
        "    if torch.cuda.get_device_properties(0).major >= 8:\n",
        "        torch.backends.cuda.matmul.allow_tf32 = True\n",
        "        torch.backends.cudnn.allow_tf32 = True\n",
        "elif device.type == \"mps\":\n",
        "    print(\"\\nSupport for MPS devices is preliminary. SAM 2 is trained with CUDA and might give numerically different outputs and sometimes degraded performance on MPS. See e.g. https://github.com/pytorch/pytorch/issues/84936 for a discussion.\")\n",
        "\n",
        "# Initialize predictor\n",
        "sam2_checkpoint = \"../checkpoints/sam2.1_hiera_large.pt\"\n",
        "model_cfg = \"configs/sam2.1/sam2.1_hiera_l.yaml\"\n",
        "predictor = build_sam2_video_predictor(model_cfg, sam2_checkpoint, device=device)\n",
        "\n",
        "# Load coordinates from CSV file\n",
        "coordinates = []\n",
        "with open('/home/totargaming/workspace/sam2/aria-handtracking/merged_output.csv', 'r') as f:\n",
        "    reader = csv.DictReader(f)\n",
        "    for row in reader:\n",
        "        coordinates.append({\n",
        "            'folder_name': row['folder_name'],\n",
        "            'hand_point_x': float(row['hand_point_x']),\n",
        "            'hand_point_y': float(row['hand_point_y']),\n",
        "            'obj_point_x': float(row['obj_point_x']),\n",
        "            'obj_point_y': float(row['obj_point_y']),\n",
        "            'box_x1': float(row['box_x1']),\n",
        "            'box_y1': float(row['box_y1']),\n",
        "            'box_x2': float(row['box_x2']),\n",
        "            'box_y2': float(row['box_y2']),\n",
        "            'frame': int(row['frame'])\n",
        "        })\n",
        "\n",
        "for coord in coordinates:\n",
        "    folder_name = coord['folder_name']\n",
        "    hand_point_x = coord['hand_point_x']\n",
        "    hand_point_y = coord['hand_point_y']\n",
        "    obj_point_x = coord['obj_point_x']\n",
        "    obj_point_y = coord['obj_point_y']\n",
        "    box_x1 = coord['box_x1']\n",
        "    box_y1 = coord['box_y1']\n",
        "    box_x2 = coord['box_x2']\n",
        "    box_y2 = coord['box_y2']\n",
        "    frame_index = coord['frame']\n",
        "\n",
        "    frame_dir = f\"./HOIST/valid/JPEGImages/{folder_name}\"\n",
        "    output_dir = f\"./collected_new\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    frame_names = sorted([p for p in os.listdir(frame_dir) if os.path.splitext(p)[-1].lower() in [\".jpg\", \".jpeg\"]], key=lambda p: int(os.path.splitext(p)[0].split('_')[-1]))\n",
        "\n",
        "    if frame_names and frame_index < len(frame_names):\n",
        "        frame_path = os.path.join(frame_dir, frame_names[frame_index])\n",
        "        frame = Image.open(frame_path)\n",
        "\n",
        "        # Initialize inference state\n",
        "        inference_state = predictor.init_state(video_path=frame_dir)\n",
        "        predictor.reset_state(inference_state)\n",
        "\n",
        "        # Add positive prompts for the hand and the object\n",
        "        points = np.array([[hand_point_x, hand_point_y], [obj_point_x, obj_point_y]], dtype=np.float32)\n",
        "        labels = np.array([1, 1], np.int32)  # 1 indicates a positive prompt\n",
        "        _, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(inference_state, frame_idx=frame_index, obj_id=1, points=points, labels=labels)\n",
        "\n",
        "        # Add the hand point as a negative prompt\n",
        "        points = np.array([[hand_point_x, hand_point_y]], dtype=np.float32)\n",
        "        labels = np.array([0], np.int32)  # 0 indicates a negative prompt\n",
        "        _, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(inference_state, frame_idx=frame_index, obj_id=1, points=points, labels=labels)\n",
        "\n",
        "        # Propagate the prompts to get the masklet across the video\n",
        "        video_segments = {out_frame_idx: {out_obj_id: (out_mask_logits[i] > 0.0).cpu().numpy() for i, out_obj_id in enumerate(out_obj_ids)} for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(inference_state)}\n",
        "\n",
        "        # Start timing\n",
        "        start_time = time.time()\n",
        "\n",
        "        mask_data = {}\n",
        "\n",
        "        def mask_to_polygon(mask):\n",
        "            # Convert mask to binary\n",
        "            mask_binary = mask.astype(bool)\n",
        "            print(f\"mask shape: {mask.shape}, mask_binary shape: {mask_binary.shape}\")\n",
        "            print(f\"mask_binary unique values: {np.unique(mask_binary)}\")\n",
        "            \n",
        "            # Check if mask is empty\n",
        "            if mask_binary.size == 0 or np.all(mask_binary == 0):\n",
        "                return []\n",
        "            \n",
        "            # Ensure mask is 2D\n",
        "            if mask_binary.ndim > 2:\n",
        "                mask_binary = mask_binary[0]\n",
        "            \n",
        "            # Find contours\n",
        "            contours = find_contours(mask_binary, level=0.5)\n",
        "            # Convert contours to polygons\n",
        "            polygons = [contour.tolist() for contour in contours]\n",
        "            return polygons\n",
        "\n",
        "        for frame_idx in range(len(frame_names)):\n",
        "            frame_masks = []\n",
        "            \n",
        "            # Process masks\n",
        "            for obj_id, mask in video_segments.get(frame_idx, {}).items():\n",
        "                polygons = mask_to_polygon(mask)\n",
        "                frame_masks.append({\n",
        "                    \"obj_id\": obj_id,\n",
        "                    \"polygons\": polygons\n",
        "                })\n",
        "            mask_data[frame_idx] = frame_masks\n",
        "            \n",
        "            gc.collect()  # Force garbage collection\n",
        "            print(f\"Processed frame {frame_idx + 1}/{len(frame_names)}\")\n",
        "\n",
        "        # Save mask data to JSON\n",
        "        with open(os.path.join(output_dir, f\"{folder_name}.json\"), \"w\") as json_file:\n",
        "            json.dump(mask_data, json_file, indent=4)\n",
        "\n",
        "        # End timing\n",
        "        end_time = time.time()\n",
        "        elapsed_time = end_time - start_time\n",
        "        print(f\"Total time taken: {elapsed_time:.2f} seconds\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
