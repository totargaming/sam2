{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "from sam2.build_sam import build_sam2_video_predictor\n",
    "\n",
    "# Set device for computation\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()\n",
    "    if torch.cuda.get_device_properties(0).major >= 8:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "elif device.type == \"mps\":\n",
    "    print(\"\\nSupport for MPS devices is preliminary. SAM 2 is trained with CUDA and might give numerically different outputs and sometimes degraded performance on MPS. See e.g. https://github.com/pytorch/pytorch/issues/84936 for a discussion.\")\n",
    "\n",
    "# Initialize predictor\n",
    "sam2_checkpoint = \"../checkpoints/sam2.1_hiera_large.pt\"\n",
    "model_cfg = \"configs/sam2.1/sam2.1_hiera_l.yaml\"\n",
    "predictor = build_sam2_video_predictor(model_cfg, sam2_checkpoint, device=device)\n",
    "\n",
    "def show_mask(mask, frame, obj_id=None, random_color=False):\n",
    "    color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0) if random_color else np.array([*plt.get_cmap(\"tab10\")(0 if obj_id is None else obj_id)[:3], 0.6])\n",
    "    mask = mask.reshape(*mask.shape[-2:], 1)\n",
    "    color = color.reshape(1, 1, -1)\n",
    "    frame[mask > 0] = frame[mask > 0] * (1 - color[3]) + color[:3] * 255 * color[3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_dir = \"./videos/aria\"\n",
    "frame_names = sorted([p for p in os.listdir(video_dir) if os.path.splitext(p)[-1].lower() in [\".jpg\", \".jpeg\"]], key=lambda p: int(os.path.splitext(p)[0]))\n",
    "inference_state = predictor.init_state(video_path=video_dir)\n",
    "predictor.reset_state(inference_state)\n",
    "\n",
    "points, labels = np.array([[1100, 900]], dtype=np.float32), np.array([1], np.int32)\n",
    "_, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(inference_state, frame_idx=0, obj_id=1, points=points, labels=labels)\n",
    "\n",
    "video_segments = {out_frame_idx: {out_obj_id: (out_mask_logits[i] > 0.0).cpu().numpy() for i, out_obj_id in enumerate(out_obj_ids)} for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(inference_state)}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
